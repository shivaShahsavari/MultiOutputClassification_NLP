{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f69f134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T20:17:02.342203Z",
     "iopub.status.busy": "2022-11-01T20:17:02.340997Z",
     "iopub.status.idle": "2022-11-01T20:17:02.357898Z",
     "shell.execute_reply": "2022-11-01T20:17:02.357051Z"
    },
    "papermill": {
     "duration": 0.025397,
     "end_time": "2022-11-01T20:17:02.360144",
     "exception": false,
     "start_time": "2022-11-01T20:17:02.334747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import scipy.cluster.hierarchy as spc\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndf_cr=df_train[['cohesion', 'syntax', 'vocabulary','phraseology', 'grammar', 'conventions']]\\ncorr = df_cr.corr()\\nround(corr,2)\\n#sns.heatmap(corr)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import scipy.cluster.hierarchy as spc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_cr=df_train[['cohesion', 'syntax', 'vocabulary','phraseology', 'grammar', 'conventions']]\n",
    "corr = df_cr.corr()\n",
    "round(corr,2)\n",
    "#sns.heatmap(corr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67112d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T20:17:02.368631Z",
     "iopub.status.busy": "2022-11-01T20:17:02.368369Z",
     "iopub.status.idle": "2022-11-01T21:01:38.435113Z",
     "shell.execute_reply": "2022-11-01T21:01:38.434183Z"
    },
    "papermill": {
     "duration": 2676.557628,
     "end_time": "2022-11-01T21:01:38.921355",
     "exception": false,
     "start_time": "2022-11-01T20:17:02.363727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 20:17:10.752049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:10.753235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:10.753932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:10.755914: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-01 20:17:10.756273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:10.757010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:10.757852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:15.726583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:15.727492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:15.728217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 20:17:15.728824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "100%|██████████| 3128/3128 [00:36<00:00, 84.81it/s]\n",
      "100%|██████████| 783/783 [00:09<00:00, 85.17it/s]\n",
      "Some layers from the model checkpoint at ../input/d/shahsavari86/bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../input/d/shahsavari86/bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 20:18:27.238861: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 122s 284ms/step - loss: 0.5826 - root_mean_squared_error: 0.7633 - val_loss: 0.2864 - val_root_mean_squared_error: 0.5352\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28640, saving model to ./mymodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 20:20:27.483126: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2595 - root_mean_squared_error: 0.5094 - val_loss: 0.2640 - val_root_mean_squared_error: 0.5138\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28640 to 0.26403, saving model to ./mymodel\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2442 - root_mean_squared_error: 0.4941 - val_loss: 0.2447 - val_root_mean_squared_error: 0.4947\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26403 to 0.24468, saving model to ./mymodel\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2360 - root_mean_squared_error: 0.4858 - val_loss: 0.2499 - val_root_mean_squared_error: 0.4999\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.24468\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2344 - root_mean_squared_error: 0.4842 - val_loss: 0.3335 - val_root_mean_squared_error: 0.5775\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24468\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2362 - root_mean_squared_error: 0.4860 - val_loss: 0.2748 - val_root_mean_squared_error: 0.5242\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24468\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2311 - root_mean_squared_error: 0.4807 - val_loss: 0.3230 - val_root_mean_squared_error: 0.5683\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24468\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2302 - root_mean_squared_error: 0.4798 - val_loss: 0.2619 - val_root_mean_squared_error: 0.5118\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24468\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2209 - root_mean_squared_error: 0.4700 - val_loss: 0.2628 - val_root_mean_squared_error: 0.5126\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24468\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 108s 277ms/step - loss: 0.2160 - root_mean_squared_error: 0.4648 - val_loss: 0.2773 - val_root_mean_squared_error: 0.5265\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24468\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2156 - root_mean_squared_error: 0.4643 - val_loss: 0.2627 - val_root_mean_squared_error: 0.5126\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24468\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 108s 277ms/step - loss: 0.2149 - root_mean_squared_error: 0.4635 - val_loss: 0.2913 - val_root_mean_squared_error: 0.5397\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.24468\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2191 - root_mean_squared_error: 0.4681 - val_loss: 0.2735 - val_root_mean_squared_error: 0.5229\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.24468\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 108s 277ms/step - loss: 0.2154 - root_mean_squared_error: 0.4641 - val_loss: 0.2820 - val_root_mean_squared_error: 0.5311\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.24468\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2117 - root_mean_squared_error: 0.4601 - val_loss: 0.2680 - val_root_mean_squared_error: 0.5177\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.24468\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 108s 277ms/step - loss: 0.2161 - root_mean_squared_error: 0.4649 - val_loss: 0.2906 - val_root_mean_squared_error: 0.5390\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.24468\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2117 - root_mean_squared_error: 0.4601 - val_loss: 0.2787 - val_root_mean_squared_error: 0.5279\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.24468\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 108s 278ms/step - loss: 0.2123 - root_mean_squared_error: 0.4608 - val_loss: 0.2611 - val_root_mean_squared_error: 0.5110\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.24468\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 108s 278ms/step - loss: 0.2058 - root_mean_squared_error: 0.4536 - val_loss: 0.2935 - val_root_mean_squared_error: 0.5418\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.24468\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 108s 278ms/step - loss: 0.2123 - root_mean_squared_error: 0.4608 - val_loss: 0.2817 - val_root_mean_squared_error: 0.5308\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.24468\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2123 - root_mean_squared_error: 0.4607 - val_loss: 0.2696 - val_root_mean_squared_error: 0.5192\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.24468\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 108s 278ms/step - loss: 0.2060 - root_mean_squared_error: 0.4539 - val_loss: 0.2807 - val_root_mean_squared_error: 0.5298\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.24468\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2097 - root_mean_squared_error: 0.4579 - val_loss: 0.2771 - val_root_mean_squared_error: 0.5264\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.24468\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "#strategy = tf.distribute.TPUStrategy(tpu)\n",
    "#!ln -s /kaggle/input/omw14/omw-1.4 /usr/share/nltk_data/corpora/omw-1.4\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.data.path.append(\"../input/omw14/omw-1.4\")\n",
    "from gensim.models import Phrases\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer,TFBertModel\n",
    "from tqdm import tqdm\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../input/feedback-prize-english-language-learning/train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_x, train_y = np.array(train_df[\"full_text\"]), np.array(train_df[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]])\n",
    "val_x, val_y = np.array(test_df[\"full_text\"]), np.array(test_df[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]])\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8 #*strategy.num_replicas_in_sync\n",
    "BUFFER_SIZE = 3200\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "NUM_TRAIN = int(len(df)*0.8)\n",
    "SEQ_LEN = 512\n",
    "CHECKPOINT_PATH = \"./mymodel\"\n",
    "EPOCHS = 100\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../input/bert-base-uncased\")\n",
    "\n",
    "def get_ids_mask(inputs, labels):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for x in tqdm(inputs):\n",
    "        tokens = tokenizer(x, padding=\"max_length\", truncation=True, max_length=SEQ_LEN, return_tensors=\"np\")\n",
    "        ids = tokens[\"input_ids\"]\n",
    "        mask = tokens[\"attention_mask\"]\n",
    "        input_ids.append(ids)\n",
    "        attention_mask.append(mask)\n",
    "    input_ids = np.array(input_ids).squeeze()\n",
    "    attention_mask = np.array(attention_mask).squeeze()\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "train_input_ids, train_attention_mask = get_ids_mask(train_x, train_y)\n",
    "val_input_ids, val_attention_mask = get_ids_mask(val_x, val_y)\n",
    "\n",
    "\n",
    "def get_train_dataset(ids, mask, y):\n",
    "    x = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\": tf.constant(ids, dtype=\"int32\"),\n",
    "        \"attention_mask\": tf.constant(mask, dtype=\"int32\")\n",
    "    })\n",
    "    y = tf.data.Dataset.from_tensor_slices(y)\n",
    "    data = tf.data.Dataset.zip((x, y))\n",
    "    data = data.repeat()\n",
    "    data = data.shuffle(BUFFER_SIZE)\n",
    "    data = data.batch(BATCH_SIZE)\n",
    "    data = data.prefetch(AUTO)\n",
    "    return data\n",
    "\n",
    "def get_val_dataset(ids, mask, y):\n",
    "    x = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\": tf.constant(ids, dtype=\"int32\"),\n",
    "        \"attention_mask\": tf.constant(mask, dtype=\"int32\")\n",
    "    })\n",
    "    y = tf.data.Dataset.from_tensor_slices(y)\n",
    "    data = tf.data.Dataset.zip((x, y))\n",
    "    data = data.batch(BATCH_SIZE)\n",
    "    data = data.prefetch(AUTO)\n",
    "    return data\n",
    "\n",
    "train_dataset = get_train_dataset(train_input_ids, train_attention_mask, train_y)\n",
    "val_dataset = get_val_dataset(val_input_ids, val_attention_mask, val_y)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "def build_model():\n",
    "    # Multi inputs\n",
    "    input1 = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"input_ids\")\n",
    "    input2 = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"attention_mask\")\n",
    "    \n",
    "    base_model = TFBertModel.from_pretrained(\"../input/d/shahsavari86/bert-base-uncased\")\n",
    "    base_model.trainable = False  # Freeze bert model\n",
    "    \n",
    "    base_outputs = base_model.bert({\"input_ids\": input1,\"attention_mask\": input2})\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(base_outputs[0])\n",
    "    outputs = tf.keras.layers.Dense(6)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs={\"input_ids\": input1,\"attention_mask\": input2}, outputs=outputs)\n",
    "   \n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "    \n",
    "    return model\n",
    "\n",
    "#with strategy.scope():\n",
    "#    model = build_model()\n",
    "deep_model = build_model()\n",
    "    \n",
    "save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH,\n",
    "                                                   monitor='val_loss',\n",
    "                                                   options=save_locally,\n",
    "                                                   save_best_only=True,\n",
    "                                                    verbose=1)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  patience=20,\n",
    "                                                  verbose=1)\n",
    "\n",
    "reducelr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                     factor=0.5,\n",
    "                                                     patience=5,\n",
    "                                                     verbose=1)\n",
    "callbacks = [model_checkpoint_callback,earlystop_callback,reducelr_callback]\n",
    "\n",
    "freeze_history = deep_model.fit(train_dataset, \n",
    "                           validation_data=val_dataset, \n",
    "                           steps_per_epoch=NUM_TRAIN//BATCH_SIZE,\n",
    "                           callbacks=callbacks, \n",
    "                           epochs=EPOCHS,\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0efe59d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T21:01:39.938938Z",
     "iopub.status.busy": "2022-11-01T21:01:39.938573Z",
     "iopub.status.idle": "2022-11-01T21:01:39.943140Z",
     "shell.execute_reply": "2022-11-01T21:01:39.942139Z"
    },
    "papermill": {
     "duration": 0.485849,
     "end_time": "2022-11-01T21:01:39.945139",
     "exception": false,
     "start_time": "2022-11-01T21:01:39.459290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#deep_model.save('/kaggle/working/Bert_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbcced34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T21:01:41.384516Z",
     "iopub.status.busy": "2022-11-01T21:01:41.383927Z",
     "iopub.status.idle": "2022-11-01T21:10:15.733639Z",
     "shell.execute_reply": "2022-11-01T21:10:15.731016Z"
    },
    "papermill": {
     "duration": 515.166115,
     "end_time": "2022-11-01T21:10:15.735969",
     "exception": false,
     "start_time": "2022-11-01T21:01:40.569854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of deep model on train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3911/3911 [06:20<00:00, 10.28it/s]\n",
      "100%|██████████| 3911/3911 [00:00<00:00, 16980.13it/s]\n",
      "100%|██████████| 3911/3911 [00:17<00:00, 226.82it/s]\n",
      "100%|██████████| 3911/3911 [00:21<00:00, 182.57it/s]\n",
      "100%|██████████| 3911/3911 [00:20<00:00, 187.07it/s]\n",
      "100%|██████████| 3911/3911 [00:53<00:00, 72.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "!ln -s /kaggle/input/omw14/omw-1.4 /usr/share/nltk_data/corpora/omw-1.4\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.data.path.append(\"../input/omw14/omw-1.4\")\n",
    "from gensim.models import Phrases\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "#deep_model = tf.keras.models.load_model('../input/my-trained-bert/Bert_model.h5')\n",
    "load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "deep_model = tf.keras.models.load_model(CHECKPOINT_PATH, options=load_locally)\n",
    "\n",
    "###second step\n",
    "def remove_stop_word(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    filtered_text=[]\n",
    "    word_tokens = word_tokenize(text)\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_text.append(w)\n",
    "    return filtered_text\n",
    "\n",
    "def lammatization(text):\n",
    "    lema_text=[]\n",
    "    filtered_text = remove_stop_word(text)\n",
    "    for w in filtered_text:\n",
    "        res = lemma.lemmatize(w)\n",
    "        lema_text.append(res)\n",
    "    return lema_text\n",
    "\n",
    "def sent_count(inputs):\n",
    "    sent_count = []\n",
    "    for x in tqdm(inputs):\n",
    "        sent_c=len(re.findall(r\"[^?!.][?!.]\", x))\n",
    "        sent_count.append(sent_c)\n",
    "    sent_count = np.array(sent_count).squeeze()\n",
    "    return sent_count\n",
    "\n",
    "def unique_word_count(inputs):\n",
    "    vocabs_count = []\n",
    "    for x in tqdm(inputs):\n",
    "        lema = lammatization(x)\n",
    "        items = Counter(lema).keys()\n",
    "        vocabs_count.append(len(items))\n",
    "    vocabs_count = np.array(vocabs_count).squeeze()\n",
    "    return vocabs_count\n",
    "\n",
    "def biphrase_count(inputs):\n",
    "    phrase_bigram=[]\n",
    "    phrase_count = []\n",
    "    for x in tqdm(inputs):\n",
    "        sentence_stream = x.split(\" \")\n",
    "        bigram = Phrases(sentence_stream, min_count=1, threshold=2)\n",
    "        filtered_text = remove_stop_word(x)\n",
    "        phrase_bigram = bigram[filtered_text]\n",
    "        items = Counter(phrase_bigram).keys()\n",
    "        phrase_count.append(len(items))\n",
    "    phrase_count = np.array(phrase_count).squeeze()    \n",
    "    return phrase_count\n",
    "\n",
    "def threephrase_count(inputs):\n",
    "    phrase_bigram=[]\n",
    "    phrase_count = []\n",
    "    for x in tqdm(inputs):\n",
    "        sentence_stream = x.split(\" \")\n",
    "        bigram = Phrases(sentence_stream, min_count=1, threshold=3)\n",
    "        filtered_text = remove_stop_word(x)\n",
    "        phrase_bigram = bigram[filtered_text]\n",
    "        items = Counter(phrase_bigram).keys()\n",
    "        phrase_count.append(len(items))\n",
    "    phrase_count = np.array(phrase_count).squeeze()    \n",
    "    return phrase_count\n",
    "\n",
    "def pos_tag_count(inputs):\n",
    "    pose_count = []\n",
    "    for x in tqdm(inputs):\n",
    "        filtered_text = remove_stop_word(x)\n",
    "        tagged_text = nltk.pos_tag(filtered_text)\n",
    "        tags = [tup[1] for tup in tagged_text]\n",
    "        item_tag = Counter(tags).keys()\n",
    "        pose_count.append(len(item_tag))\n",
    "    pose_count = np.array(pose_count).squeeze() \n",
    "    return pose_count\n",
    "\n",
    "\n",
    "print('Prediction of deep model on train data')\n",
    "train_x = np.array(df[\"full_text\"])\n",
    "predictions_x = []\n",
    "for x in tqdm(train_x):\n",
    "    token = tokenizer(x, padding=\"max_length\", truncation=True, max_length=SEQ_LEN, return_tensors=\"np\")\n",
    "    ids = token[\"input_ids\"]\n",
    "    mask = token[\"attention_mask\"]\n",
    "    prediction = deep_model.predict({\"input_ids\": ids,\"attention_mask\": mask})\n",
    "    predictions_x.append(prediction[0])\n",
    "\n",
    "pred_df=pd.DataFrame(predictions_x)\n",
    "df_train_x = pd.DataFrame()\n",
    "df_train_x['cohesion_x'] = ((round(pred_df.iloc[:,0])*2)/2).astype(int)\n",
    "df_train_x['syntax_x'] = ((round(pred_df.iloc[:,1])*2)/2).astype(int)\n",
    "df_train_x['vocabulary_x'] = ((round(pred_df.iloc[:,2])*2)/2).astype(int)\n",
    "df_train_x['phraseology_x'] = ((round(pred_df.iloc[:,3])*2)/2).astype(int)\n",
    "df_train_x['grammar_x'] = ((round(pred_df.iloc[:,4])*2)/2).astype(int)\n",
    "df_train_x['conventions_x'] = ((round(pred_df.iloc[:,5])*2)/2).astype(int)\n",
    "\n",
    "\n",
    "df_train_x[[\"cohesion_x\",\"syntax_x\",\"vocabulary_x\",\"phraseology_x\",\"grammar_x\",\"conventions_x\"]] = predictions_x\n",
    "df_train_x['sent_count'] = sent_count(df['full_text'])\n",
    "df_train_x['vocabs_count'] = unique_word_count(df['full_text'])\n",
    "df_train_x['biphrase_count'] = biphrase_count(df['full_text'])\n",
    "df_train_x['threephrase_count'] = threephrase_count(df['full_text'])\n",
    "df_train_x['pose_count'] = pos_tag_count(df['full_text'])\n",
    "df_train_y = df[['cohesion','syntax','vocabulary','phraseology','grammar','conventions']]\n",
    "df_train_y = df_train_y.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f96d4a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T21:10:17.073932Z",
     "iopub.status.busy": "2022-11-01T21:10:17.073549Z",
     "iopub.status.idle": "2022-11-01T21:10:17.083176Z",
     "shell.execute_reply": "2022-11-01T21:10:17.081909Z"
    },
    "papermill": {
     "duration": 0.651077,
     "end_time": "2022-11-01T21:10:17.086734",
     "exception": false,
     "start_time": "2022-11-01T21:10:16.435657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
            ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor\n",
    "\n",
    "result=[]\n",
    "\n",
    "print('model_1')\n",
    "clf1 = LinearRegression()\n",
    "param1 = {}\n",
    "#param1['alpha'] = [1, 10, 100, 290, 500]\n",
    "param1['fit_intercept'] = [True, False]\n",
    "#param1['normalize'] = [True, False]\n",
    "gs = GridSearchCV(clf1, param1, cv=5, n_jobs=-1).fit(df_train_x, df_train_y)\n",
    "result.append({ 'grid': gs,\n",
    "                'classifier': gs.best_estimator_,\n",
    "                'best score': gs.best_score_,\n",
    "                'best params': gs.best_params_,\n",
    "                'cv': gs.cv })\n",
    "\n",
    "print('model_2')\n",
    "clf2 = KNeighborsRegressor()\n",
    "param2 = {}\n",
    "param2['n_neighbors'] = [2,3,4,5,6,7,8,9,10]\n",
    "param2['weights'] = ['uniform','distance']\n",
    "param2['p'] = [1,2,5]\n",
    "gs = GridSearchCV(clf2, param2, cv=5, n_jobs=-1).fit(df_train_x, df_train_y)\n",
    "result.append({ 'grid': gs,\n",
    "                'classifier': gs.best_estimator_,\n",
    "                'best score': gs.best_score_,\n",
    "                'best params': gs.best_params_,\n",
    "                'cv': gs.cv })\n",
    "\n",
    "print('model_3')\n",
    "clf3 = DecisionTreeRegressor()\n",
    "param3 = {}\n",
    "param3['min_samples_split'] = [2,3,4,5,6,7,8,9,10]\n",
    "gs = GridSearchCV(clf3, param3, cv=5, n_jobs=-1).fit(df_train_x, df_train_y)\n",
    "result.append({ 'grid': gs,\n",
    "                'classifier': gs.best_estimator_,\n",
    "                'best score': gs.best_score_,\n",
    "                'best params': gs.best_params_,\n",
    "                'cv': gs.cv })\n",
    "\n",
    "\n",
    "print('model_4')\n",
    "clf4 = RandomForestRegressor()\n",
    "param4 = {}\n",
    "gs = GridSearchCV(clf4, param4, cv=5, n_jobs=-1).fit(df_train_x, df_train_y)\n",
    "result.append({ 'grid': gs,\n",
    "                'classifier': gs.best_estimator_,\n",
    "                'best score': gs.best_score_,\n",
    "                'best params': gs.best_params_,\n",
    "                'cv': gs.cv })\n",
    "\n",
    "\n",
    "print('model_5')\n",
    "clf5 = MultiOutputRegressor(LinearSVR(max_iter=10000))\n",
    "param5 = {}\n",
    "gs = GridSearchCV(clf5, param5, cv=5, n_jobs=-1).fit(df_train_x, df_train_y)\n",
    "result.append({ 'grid': gs,\n",
    "                'classifier': gs.best_estimator_,\n",
    "                'best score': gs.best_score_,\n",
    "                'best params': gs.best_params_,\n",
    "                'cv': gs.cv })\n",
    "\n",
    "print('model_6')\n",
    "clf6 = MultiOutputRegressor(GradientBoostingRegressor())\n",
    "param6 = {}\n",
    "gs = GridSearchCV(clf6, param6, cv=5, n_jobs=-1).fit(df_train_x, df_train_y)\n",
    "result.append({ 'grid': gs,\n",
    "                'classifier': gs.best_estimator_,\n",
    "                'best score': gs.best_score_,\n",
    "                'best params': gs.best_params_,\n",
    "                'cv': gs.cv })\n",
    "\n",
    "print('model_7')\n",
    "clf7 = MultiOutputRegressor(AdaBoostRegressor())\n",
    "param7 = {}\n",
    "gs = GridSearchCV(clf7, param7, cv=5, n_jobs=-1).fit(df_train_x, df_train_y)\n",
    "result.append({ 'grid': gs,\n",
    "                'classifier': gs.best_estimator_,\n",
    "                'best score': gs.best_score_,\n",
    "                'best params': gs.best_params_,\n",
    "                'cv': gs.cv })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ff2848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T21:10:18.403576Z",
     "iopub.status.busy": "2022-11-01T21:10:18.403191Z",
     "iopub.status.idle": "2022-11-01T21:10:18.412344Z",
     "shell.execute_reply": "2022-11-01T21:10:18.411379Z"
    },
    "papermill": {
     "duration": 0.642939,
     "end_time": "2022-11-01T21:10:18.414382",
     "exception": false,
     "start_time": "2022-11-01T21:10:17.771443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
      "[{'grid': GridSearchCV(cv=5, estimator=LinearRegression(), n_jobs=-1,\n",
    "             param_grid={'fit_intercept': [True, False]}), 'classifier': LinearRegression(fit_intercept=False), 'best score': 0.431213246883791, 'best params': {'fit_intercept': False}, 'cv': 5}, {'grid': GridSearchCV(cv=5,\n",
    "             estimator=MultiOutputRegressor(estimator=GradientBoostingRegressor()),\n",
    "             n_jobs=-1, param_grid={}), 'classifier': MultiOutputRegressor(estimator=GradientBoostingRegressor()), 'best score': 0.4176872586575013, 'best params': {}, 'cv': 5}, {'grid': GridSearchCV(cv=5,\n",
    "             estimator=MultiOutputRegressor(estimator=AdaBoostRegressor()),\n",
    "             n_jobs=-1, param_grid={}), 'classifier': MultiOutputRegressor(estimator=AdaBoostRegressor()), 'best score': 0.4057761192685268, 'best params': {}, 'cv': 5}, {'grid': GridSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1, param_grid={}), 'classifier': RandomForestRegressor(), 'best score': 0.398965649662622, 'best params': {}, 'cv': 5}, {'grid': GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
    "             param_grid={'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                         'p': [1, 2, 5], 'weights': ['uniform', 'distance']}), 'classifier': KNeighborsRegressor(n_neighbors=10, p=1, weights='distance'), 'best score': 0.23786437988714462, 'best params': {'n_neighbors': 10, 'p': 1, 'weights': 'distance'}, 'cv': 5}, {'grid': GridSearchCV(cv=5, estimator=DecisionTreeRegressor(), n_jobs=-1,\n",
    "             param_grid={'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}), 'classifier': DecisionTreeRegressor(min_samples_split=10), 'best score': 0.11364804048849426, 'best params': {'min_samples_split': 10}, 'cv': 5}, {'grid': GridSearchCV(cv=5,\n",
    "             estimator=MultiOutputRegressor(estimator=LinearSVR(max_iter=10000)),\n",
    "             n_jobs=-1, param_grid={}), 'classifier': MultiOutputRegressor(estimator=LinearSVR(max_iter=10000)), 'best score': 0.112735980021841, 'best params': {}, 'cv': 5}]\n"
 
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "result = sorted(result, key=itemgetter('best score'),reverse=True)\n",
    "print(result)\n",
    "\n",
       ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57536ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T21:10:22.395474Z",
     "iopub.status.busy": "2022-11-01T21:10:22.395055Z",
     "iopub.status.idle": "2022-11-01T21:10:22.960775Z",
     "shell.execute_reply": "2022-11-01T21:10:22.959636Z"
    },
    "papermill": {
     "duration": 1.259329,
     "end_time": "2022-11-01T21:10:22.963615",
     "exception": false,
     "start_time": "2022-11-01T21:10:21.704286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of deep model on test data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  8.30it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6563.86it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 224.57it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 155.87it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 147.42it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 56.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_boost = LinearRegression(fit_intercept=False)\n",
    "model_boost.fit(df_train_x, df_train_y)\n",
    "#r_sq_model_boost = model_boost.score(df_train_x, df_train_y)\n",
    "#print(f\"coefficient of determination model booster: {r_sq_model_boost}\") #0.9191241450168699\n",
    "\n",
    "print('Prediction of deep model on test data:')\n",
    "df_test = pd.read_csv(\"../input/feedback-prize-english-language-learning/test.csv\")\n",
    "test_x = np.array(df_test[\"full_text\"])\n",
    "predictions = []\n",
    "for x in tqdm(test_x):\n",
    "    token = tokenizer(x, padding=\"max_length\", truncation=True, max_length=SEQ_LEN, return_tensors=\"np\")\n",
    "    ids = token[\"input_ids\"]\n",
    "    mask = token[\"attention_mask\"]\n",
    "    prediction = deep_model.predict({\"input_ids\": ids,\n",
    "                               \"attention_mask\": mask})\n",
    "    predictions.append(prediction[0])\n",
    "\n",
    "pred_test=pd.DataFrame(predictions)\n",
    "df_test['cohesion_x'] = ((round(pred_test.iloc[:,0])*2)/2).astype(int)\n",
    "df_test['syntax_x'] = ((round(pred_test.iloc[:,1])*2)/2).astype(int)\n",
    "df_test['vocabulary_x'] = ((round(pred_test.iloc[:,2])*2)/2).astype(int)\n",
    "df_test['phraseology_x'] = ((round(pred_test.iloc[:,3])*2)/2).astype(int)\n",
    "df_test['grammar_x'] = ((round(pred_test.iloc[:,4])*2)/2).astype(int)\n",
    "df_test['conventions_x'] = ((round(pred_test.iloc[:,5])*2)/2).astype(int)\n",
    "\n",
    "\n",
    "df_test[[\"cohesion_x\",\"syntax_x\",\"vocabulary_x\",\"phraseology_x\",\"grammar_x\",\"conventions_x\"]] = predictions\n",
    "df_test['sent_count'] = sent_count(df_test['full_text'])\n",
    "df_test['vocabs_count'] = unique_word_count(df_test['full_text'])\n",
    "df_test['biphrase_count'] = biphrase_count(df_test['full_text'])\n",
    "df_test['threephrase_count'] = threephrase_count(df_test['full_text'])\n",
    "df_test['pose_count'] = pos_tag_count(df_test['full_text'])\n",
    "\n",
    "y_pred_model = model_boost.predict(df_test[[\"cohesion_x\",\"syntax_x\",\"vocabulary_x\",\"phraseology_x\",\"grammar_x\",\n",
    "                                            \"conventions_x\",'sent_count','vocabs_count','biphrase_count',\n",
    "                                            'threephrase_count','pose_count']])\n",
    "df_test[['cohesion','syntax','vocabulary','phraseology','grammar','conventions']] = y_pred_model\n",
    "submission = df_test[['text_id','cohesion','syntax','vocabulary','phraseology','grammar','conventions']]\n",
    "\n",
    "submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdaf10c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T21:10:24.292856Z",
     "iopub.status.busy": "2022-11-01T21:10:24.292473Z",
     "iopub.status.idle": "2022-11-01T21:10:24.309712Z",
     "shell.execute_reply": "2022-11-01T21:10:24.308593Z"
    },
    "papermill": {
     "duration": 0.664011,
     "end_time": "2022-11-01T21:10:24.311937",
     "exception": false,
     "start_time": "2022-11-01T21:10:23.647926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>2.534555</td>\n",
       "      <td>2.329487</td>\n",
       "      <td>2.815894</td>\n",
       "      <td>2.542266</td>\n",
       "      <td>2.230805</td>\n",
       "      <td>2.238814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.421539</td>\n",
       "      <td>2.269103</td>\n",
       "      <td>2.642488</td>\n",
       "      <td>2.255740</td>\n",
       "      <td>2.049116</td>\n",
       "      <td>2.437496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.386451</td>\n",
       "      <td>3.285970</td>\n",
       "      <td>3.366014</td>\n",
       "      <td>3.356526</td>\n",
       "      <td>3.204828</td>\n",
       "      <td>3.404162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  2.534555  2.329487    2.815894     2.542266  2.230805   \n",
       "1  000BAD50D026  2.421539  2.269103    2.642488     2.255740  2.049116   \n",
       "2  00367BB2546B  3.386451  3.285970    3.366014     3.356526  3.204828   \n",
       "\n",
       "   conventions  \n",
       "0     2.238814  \n",
       "1     2.437496  \n",
       "2     3.404162  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dad62",
   "metadata": {
    "papermill": {
     "duration": 0.773752,
     "end_time": "2022-11-01T21:10:25.850390",
     "exception": false,
     "start_time": "2022-11-01T21:10:25.076638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3215.223929,
   "end_time": "2022-11-01T21:10:29.742854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-01T20:16:54.518925",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
